---
import SocialList from "@/components/SocialList.astro";
// import PostPreview from "@/components/blog/PostPreview.astro";
// import { Icon } from "astro-icon/components";
// import { Image } from 'astro:assets';
import PageLayout from "@/layouts/Base.astro";
import pub1Image from "../assets/pub1.png";
import pub2Image from "../assets/pub2.png";
import pub3Image from "../assets/pub3.gif";
import pub4Image from "../assets/pub4.png";
import pub5Image from "../assets/pub5.png";
import { Icon } from "astro-icon/components";

// import { getAllPosts, sortMDByDate } from "@/data/post";
// const MAX_POSTS = 10;
// const allPosts = await getAllPosts();
// const allPostsByDate = sortMDByDate(allPosts).slice(0, MAX_POSTS);

const cactusTech = [
	{
		author: ["Xu Pan", "Zimin Xia", "Xianwei Zheng*"],
		title: "Scale-aware Co-visible Region Detection for Image Matching",
		booktitle: "ISPRS Journal of Photogrammetry and Remote Sensing",
		time: "2025",
		doi: "https://doi.org/10.1016/j.isprsjprs.2025.08.015",
		paper: "https://www.sciencedirect.com/science/article/abs/pii/S0924271625003260",
		code: "https://github.com/Geo-Tell/SCoDe",
		image: pub1Image.src,
		venueIcon: "simple-icons:elsevier",
		venueColor: "#FF6C00",
		abstract:
			"Matching images with significant scale differences remains a persistent challenge in photogrammetry and remote sensing. The scale discrepancy often degrades appearance consistency and introduces uncertainty in keypoint localization. While existing methods address scale variation through scale pyramids or scale-aware training, matching under significant scale differences remains an open challenge. To overcome this, we address the scale difference issue by detecting co-visible regions between image pairs and propose SCoDe (Scale-aware Co-visible region Detector), which both identifies co-visible regions and aligns their scales for highly robust, hierarchical point correspondence matching. Specifically, SCoDe employs a novel Scale Head Attention mechanism to map and correlate features across multiple scale subspaces, and uses a learnable query to aggregate scale-aware information of both images for co-visible region detection. In this way, correspondences can be established in a coarse-to-fine hierarchy, thereby mitigating semantic and localization uncertainties. Extensive experiments on three challenging datasets demonstrate that SCoDe outperforms state-of-the-art methods, improving the precision of a modern local feature matcher by 8.41%. Notably, SCoDe shows a clear advantage when handling images with drastic scale variations.",
	},
	{
		author: ["Xu Pan", "Qiyuan Ma", "Jintao Zhang", "Xianwei Zheng*"],
		title: "SAMatcher: Segment Anything Co-visible for Robust Feature Matching",
		booktitle: "(In Preparation)",
		time: "2026",
		doi: "",
		paper: "",
		code: "",
		image: pub2Image.src,
		venueIcon: "simple-icons:latex",
		venueColor: "#008080",
		abstract: "",
	},
	{
		author: ["Xu Pan", "Zhenglin Wan", "Xingrui Yu*"],
		title: "SG-VLA: Spatially Grounded Vision-Language-Action Learning via Dense Flow Policy Optimization",
		booktitle: "(In Preparation)",
		time: "2026",
		doi: "",
		paper: "",
		code: "",
		image: pub3Image.src,
		venueIcon: "simple-icons:latex",
		venueColor: "#008080",
		abstract: "",
	},
	{
		author: ["Xu Pan", "Xianwei Zheng*"],
		title: "Research on Large-Scale Disparity Image Matching Method Guided by Co-Visible Region",
		booktitle: "Master's Thesis",
		time: "2026",
		doi: "",
		paper: "",
		code: "",
		image: pub4Image.src,
		venueIcon: "simple-icons:wikiversity",
		venueColor: "#00649A",
		abstract:"",
	},
	{
		author: ["Jiashen Huang", "Xu Pan"],
		title: "The Institutional Filter: How Trust Shapes Inequalities Between Domestic and Global AI Models",
		booktitle: "(Under Review)",
		// booktitle: "Annual International Communication Association (ICA) Conference",
		time: "2026",
		doi: "",
		paper: "",
		code: "",
		image: pub5Image.src,
		venueIcon: "simple-icons:acm",
		venueColor: "#0085CA",
		abstract:"Artificial intelligence is increasingly woven into the way people communicate, think, and make decisions. Yet trust in AI does not grow evenly across contexts; it carries traces of national identity, institutional credibility, and emotional attachment. This study examines how institutional trust shapes user trust in domestic (DeepSeek) and global (ChatGPT) large language models (LLMs) in China. Specifically, it distinguishes between cognitive and affective dimensions of trust. Using survey data from 405 participants, we found that higher institutional trust strengthens emotional confidence in domestic AI models, while at low levels of institutional trust, this domestic advantage in perceived competence disappears. By examining the relationship between institutional trust and AI adoption, this study deepens theoretical insights into global communication inequalities in the digital era. The findings suggest that institutional trust operates as a social resource, channeling legitimacy into technological trust, thus contributing to the uneven distribution of trust in AI technologies across different societal groups. The findings offer policy insights for inclusive AI governance and the promotion of global technological equity.",
	},
];
/* Dynamically gather experience logos */
const experienceMeta: Record<string, { name: string; link: string }> = {
	"exp1.png": {
		name: "Wuhan University",
		link: "https://en.whu.edu.cn/",
	},
	"exp2.png": {
		name: "School of Remote Sensing and Information Engineering",
		link: "https://rsgis.whu.edu.cn/English/Home.htm",
	},
	"exp3.png": {
		name: "State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS)",
		link: "https://liesmars.whu.edu.cn/",
	},
	"exp4.png": {
		name: "International Technology R&D Department, Baidu, Inc.",
		link: "https://www.baidu.com/",
	},
	"exp5.png": {
		name: "Centre for Frontier AI Research (CFAR), \nInstitute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR)",
		link: "https://www.a-star.edu.sg/cfar",
	},
};

const expImages = import.meta.glob("../assets/exps/*.{png,jpg,jpeg,svg,webp}", {
	eager: true,
	import: "default",
});

// Preserve intended display order (fallback to filename order if new files are added)
const order = ["exp1.png", "exp2.png", "exp3.png", "exp4.png", "exp5.png"];

const experiences = Object.entries(expImages)
	.map(([path, mod]: [string, any]) => {
		const file = path.split("/").pop() || "";
		const meta = experienceMeta[file] || { name: file, link: "#" };

		const altTitleName = meta.name.replace(/\s*\n\s*/g, " ");
		const tooltipHtml = escapeHtml(meta.name).replaceAll("\n", "<br/>");

		return {
			file,
			src: (mod as any).src,
			name: meta.name,
			altTitleName,
			tooltipHtml,
			link: meta.link,
		};
	})
	.sort((a, b) => {
		const ai = order.indexOf(a.file);
		const bi = order.indexOf(b.file);
		if (ai === -1 && bi === -1) return a.file.localeCompare(b.file);
		if (ai === -1) return 1;
		if (bi === -1) return -1;
		return ai - bi;
	});
/* add: import all logos from assets/logos */
const logoImages = import.meta.glob("../assets/logos/*.{png,svg,jpg,jpeg,webp}", {
	eager: true,
	import: "default",
});

// helper: find logo by substring in filename
function findLogo(sub: string) {
	for (const path of Object.keys(logoImages)) {
		if (path.toLowerCase().includes(sub)) {
			const mod: any = (logoImages as any)[path];
			return mod?.src || null;
		}
	}
	return null;
}

const logoAstarSrc = findLogo("astar");
const logoBaiduSrc = findLogo("baidu");
const logoWhuSrc = findLogo("whu") || findLogo("wuhan");
const logoLiesmarsSrc = findLogo("liesmars") || findLogo("liesmars"); // fallback kept for clarity

// add: (optional) escape helper for plain-text fallback
function escapeHtml(s: string) {
	return s
		.replaceAll("&", "&amp;")
		.replaceAll("<", "&lt;")
		.replaceAll(">", "&gt;")
		.replaceAll('"', "&quot;")
		.replaceAll("'", "&#039;");
}

// update: News items now support html for flexible content
const newsItems: Array<{
	date: string;
	html?: string;
	text?: string;
	href?: string;
	label?: string;
}> = [
	{
		date: "Nov 2025",
		text: "Completed the proposal defense for my Master’s thesis.",
	},
	{
		date: "Aug 2025",
		html: `First paper accepted by <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>.
		      <a class="cactus-link" href="https://www.sciencedirect.com/science/article/abs/pii/S0924271625003260" target="_blank" rel="noopener">Paper</a>`,
	},
	{
		date: "Aug 2025",
		html: `Started a remote research internship at <strong>CFAR, A*STAR</strong>, supervised by
		      <a class="cactus-link" href="https://xingruiyu.github.io/">Dr. Xingrui Yu</a>,
		      in collaboration with <a class="cactus-link" href="https://vanzll.github.io/">Zhenglin Wan</a>.`,
	},
	{
		date: "Jul 2025",
		html: `Attended the <em>2025 Annual Academic Conference on Photogrammetry and Remote Sensing, CSGPC</em> in Kunming.`,
	},
	{
		date: "Dec 2024",
		html: `Started a research internship at <strong>Baidu</strong> in Shenzhen, 
			  supervised by <a class="cactus-link" href="https://scholar.google.com/citations?user=dcakOP4AAAAJ">Dr. Yan Zhang</a>, 
			  exploring SOTA text-to-image and text-to-video generation.`,
	},
];
---

<PageLayout meta={{ title: "Home" }}>
	<section>
		<h1 class="title mb-6">Hello World!</h1>
		<p class="mb-4">
			Hi, I am a M.Sc. student at 
			{
				logoWhuSrc && (
					<img
						src={logoWhuSrc}
						alt="Wuhan University logo"
						class="mr-1 inline h-5 w-auto object-contain align-top"
					/>
				)
			}<a class="cactus-link" href="https://en.whu.edu.cn/">Wuhan University</a>, working at the 
			{
				logoLiesmarsSrc && (
					<img
						src={logoLiesmarsSrc}
						alt="LIESMARS logo"
						class="mr-1 inline h-4 w-auto object-contain align-top"
					/>
				)
			}<a class="cactus-link" href="https://liesmars.whu.edu.cn/"
				>State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing
				(LIESMARS)</a
			>, under the guidance of
			<a class="cactus-link" href="https://jszy.whu.edu.cn/zhengxianwei/zh_CN/index.htm"
				>Prof. Xianwei Zheng</a
			>.
			My research focuses on <em class="highlight">embodied intelligence</em> and <em class="highlight">3D visual perception</em>, 
			with an emphasis on how spatial representations support generalizable decision-making and agent-centric policy learning.
		</p>
		<p class="mb-4">
			My current work lies at the intersection of <em class="highlight">computer vision</em>, <em class="highlight">reinforcement learning</em>, and <em class="highlight">generative modeling</em>, 
			where I study how 2D and 3D representations can be unified to enable robust perception–action coupling. 
			I am particularly interested in <em class="highlight">structure-aware visual representations</em> that support cross-view understanding, generalization across environments, and interaction-driven learning in embodied settings.
		</p>
		<p class="mb-4">
			Previously, I explored generative AI for image and video synthesis during my internship at 
			{
				logoBaiduSrc && (
					<img
						src={logoBaiduSrc}
						alt="Baidu logo"
						class="mr-1 inline h-4 w-auto object-contain align-top"
					/>
				)
			}<a class="cactus-link" href="https://www.baidu.com/"
				>Baidu</a
			>. 
			I am currently a remote research intern at the {
				logoAstarSrc && (
					<img
						src={logoAstarSrc}
						alt="A*STAR logo"
						class="mr-1 inline h-4 w-auto object-contain align-top"
					/>
				)
			}<a class="cactus-link" href="https://www.a-star.edu.sg/cfar"
				>Centre for Frontier AI Research (CFAR), Agency for Science, Technology and Research
				(A*STAR)</a
			>, supervised by
			<a class="cactus-link" href="https://xingruiyu.github.io/">Dr. Xingrui Yu</a>, 
			where I work on <em class="highlight">generalizable reinforcement learning for embodied agents</em>, with a focus on agent-centric formulations and transferable policies grounded in implicit spatial representations that support generalization across tasks and scenes.
		</p>
		<p class="mb-4">
			More broadly, my goal is to develop spatially grounded learning frameworks that bridge perception, geometry, and control, 
			advancing the next generation of embodied systems that can reason about and act within complex real-world environments.
		</p>
		<SocialList />
	</section>

	<section class="mt-12" aria-label="News">
		<h2 class="title mb-4 text-xl">News</h2>
		<ul class="space-y-3 h-48 overflow-y-auto pr-2">
			{newsItems.map((item) => (
				<li class="flex gap-3">
					<span class="w-[5.5rem] shrink-0 text-sm font-medium text-gray-600 dark:text-gray-400">
						{item.date}
					</span>

					<div class="min-w-0 text-sm text-gray-700 dark:text-gray-300">
						{item.html ? (
							// NOTE: html is rendered as-is; only put trusted content here
							<span set:html={item.html} />
						) : (
							<>
								{item.text && <span set:html={escapeHtml(item.text)} />}
								{item.href && (
									<>
										{" "}
										<a class="cactus-link" href={item.href} target="_blank" rel="noopener">
											{item.label ?? "More"}
										</a>
									</>
								)}
							</>
						)}
					</div>
				</li>
			))}
		</ul>
	</section>

	<section class="mt-12" aria-label="Experiences">
		<h2 class="title mb-4 text-xl">Experiences</h2>
		<div class="flex flex-wrap items-center gap-6">
			{
				experiences.map((exp) => (
					<a
						href={exp.link}
						target="_blank"
						rel="noopener"
						class="group relative flex items-center justify-center p-1"
					>
						<img
							src={exp.src}
							alt={exp.altTitleName}
							title={exp.altTitleName}
							class="h-16 w-auto object-contain transition-transform duration-200 group-hover:scale-105"
							loading="lazy"
						/>
						<span class="pointer-events-none absolute -bottom-7 left-1/2 -translate-x-1/2 whitespace-nowrap rounded bg-gray-800 px-2 py-0.5 text-[10px] font-medium leading-snug tracking-wide text-white opacity-0 shadow transition-opacity group-hover:opacity-100 text-center">
							<span set:html={exp.tooltipHtml} />
						</span>
					</a>
				))
			}
		</div>
		{/* add: collaborators / acknowledgements */}
		<br>
		<p class="mb-4 text-sm leading-relaxed text-gray-700 dark:text-gray-300">
			<b>Acknowledgements:</b><br>
			I’m grateful to my collaborators and mentors for their guidance and support, especially
			<a class="cactus-link" href="https://jszy.whu.edu.cn/zhengxianwei/zh_CN/index.htm">
				Prof. Xianwei Zheng</a>,
			<a class="cactus-link" href="https://jszy.whu.edu.cn/xionghanjiang/zh_CN/index/526145/list/">
				Prof. Hanjiang Xiong</a>,
			<a class="cactus-link" href="https://xingruiyu.github.io/">
				Dr. Xingrui Yu (A*STAR)</a>, 
			<a class="cactus-link" href="https://ziminxia.github.io/">
				Dr. Zimin Xia (EPFL)</a>,
			<a class="cactus-link" href="https://scholar.google.com/citations?user=dcakOP4AAAAJ">
				Dr. Yan Zhang (Baidu)</a>,
			and my colleagues/peers including
			<a class="cactus-link" href="https://vanzll.github.io/">
				Zhenglin Wan (NUS)</a>, 
			<a class="cactus-link" href="https://jiashenhuang.github.io/">
				Jiashen Huang (NTU)</a>, 
			Qiyuan Ma, Jintao Zhang, 
			<a class="cactus-link" href="https://zhaocy-ai.github.io/">
				Chenyu Zhao</a>, 
			and others I’ve had the pleasure to work with.
		</p>
	</section>
	<section class="mt-16">
		<h2 class="title mb-4 text-xl">Publications</h2>
		<dl class="space-y-4">
			{
				cactusTech.map((pub) => {
					const {
						author,
						title,
						booktitle,
						time,
						doi,
						paper,
						code,
						abstract,
						venueIcon,
						venueColor,
						image,
					} = pub;
					const slug = title
						.toLowerCase()
						.replace(/\s+/g, "-")
						.replace(/[^a-z0-9\-]/g, "");
					return (
						<article class="pub-article group relative -ml-2 w-full overflow-hidden rounded-xl border border-zinc-200/70 bg-white/80 p-4 shadow-sm backdrop-blur transition hover:shadow-md supports-[backdrop-filter]:bg-white/70 dark:border-zinc-800/80 dark:bg-zinc-900/80">
							{/* Use flow layout on large screens: text column grows, thumbnail has fixed size.
							   Small screens keep a single-column grid and the thumbnail remains hidden. */}
							<div class="grid grid-cols-1 gap-4 lg:flex lg:items-start lg:gap-6">
								{/* text column: allow shrinking/truncation */}
								<div class="min-w-0 lg:flex-1">
									<h3 class="text-base font-semibold leading-snug tracking-tight">
										<span class="text-accent">
											<strong>{title}</strong>
										</span>
									</h3>
									<p class="mt-1 text-sm text-gray-700 dark:text-gray-300">
										{author.map((name, index) => (
											<span style={{ fontWeight: name === "Xu Pan" ? "bold" : "normal" }}>
												{name}{index < author.length - 1 && ","}
											</span>
										))}
									</p>
									<div class="mt-2 flex flex-wrap gap-2">
										<span class="inline-flex items-center rounded-full bg-gray-100 px-2 py-0.5 text-[11px] font-medium text-gray-700 dark:bg-zinc-800 dark:text-gray-300">
											{/* render venue icon if available */}
											{venueIcon && (
												<Icon
													name={venueIcon}
													width="14"
													height="14"
													class="mr-1 inline-block align-[-0.2em]"
													style={`color: ${venueColor}`}
													aria-hidden="true"
												/>
											)}
											{booktitle}
										</span>
										{time && (
											<span class="rounded-full bg-gray-100 px-2 py-0.5 text-[11px] font-medium text-gray-700 dark:bg-zinc-800 dark:text-gray-300">
												{time}
											</span>
										)}
									</div>
									<div class="mt-3 flex flex-wrap gap-2" aria-label="Publication actions">
										{abstract && (
											<button
												type="button"
												class="abstract-btn cactus-link inline-flex items-center rounded border border-zinc-300 px-2.5 py-1 text-xs font-medium text-gray-700 transition hover:bg-accent/10 dark:border-zinc-700 dark:text-gray-300"
												data-abstract-id={`abstract-${slug}`}
												aria-expanded="false"
											>
												ABSTRACT
												<svg
													class="chev ml-2 h-3 w-3 transform transition-transform duration-200"
													viewBox="0 0 20 20"
													fill="none"
													xmlns="http://www.w3.org/2000/svg"
													aria-hidden="true"
												>
													<path
														d="M6 8l4 4 4-4"
														stroke="currentColor"
														stroke-width="1.6"
														stroke-linecap="round"
														stroke-linejoin="round"
													/>
												</svg>
											</button>
										)}
										{code && (
											<a
												class="cactus-link inline-flex items-center rounded border border-zinc-300 px-2.5 py-1 text-xs font-medium text-gray-700 transition hover:bg-accent/10 dark:border-zinc-700 dark:text-gray-300"
												href={code}
												target="_blank"
												rel="noopener"
											>
												CODE
											</a>
										)}
										{doi && (
											<a
												class="cactus-link inline-flex items-center rounded border border-zinc-300 px-2.5 py-1 text-xs font-medium text-gray-700 transition hover:bg-accent/10 dark:border-zinc-700 dark:text-gray-300"
												href={doi}
												target="_blank"
												rel="noopener"
											>
												DOI
											</a>
										)}
										{paper && (
											<a
												class="cactus-link inline-flex items-center rounded border border-zinc-300 px-2.5 py-1 text-xs font-medium text-gray-700 transition hover:bg-accent/10 dark:border-zinc-700 dark:text-gray-300"
												href={paper}
												target="_blank"
												rel="noopener"
											>
												PAPER
											</a>
										)}
									</div>
								</div>
								{/* thumbnail column (part of flow on large screens). Keep hidden on small screens. */}
								{image && (
									<div class="pub-thumb hidden lg:ml-2 lg:block">
										<button
											type="button"
											class="pub-thumb-btn group block cursor-zoom-in"
											aria-label="Open image"
										>
											<img
												src={image}
												alt="[Thumbnail]"
												class="pub-thumb-img h-[96px] w-[176px] rounded-xl border-2 border-transparent object-cover shadow-md transition duration-200 group-hover:scale-[1.04] group-hover:border-dashed group-hover:border-accent group-hover:shadow-lg"
												loading="lazy"
												decoding="async"
											/>
										</button>
									</div>
								)}
							</div>
							{/* 抽出的摘要区域：宽度为整个卡片（在小/大屏均为 full width） */}
							{abstract && (
								<div
									class="abstract-collapse mt-3 w-full overflow-hidden text-sm text-gray-700 transition-[max-height,opacity] duration-300 ease-in-out dark:text-gray-300"
									id={`abstract-content-${slug}`}
									style="max-height: 0; opacity: 0;"
									data-collapsed="true"
								>
									{abstract}
								</div>
							)}
						</article>
					);
				})
			}
		</dl>
	</section>
	<section class="mt-12" aria-label="Personal Philosophy">
		<h2 class="title mb-4 text-xl">Personal Philosophy</h2>
		<p class="mb-4">
			I follow Stoic philosophy. Life is a joyful ascent: a true mountaineer delights in the climb
			itself, not just the summit.
		</p>
		<figure class="relative border-l-4 border-accent pl-4">
			<blockquote class="italic">
				“Thou sufferest this justly: for thou choosest rather to become good to-morrow than to be
				good to-day.”
			</blockquote>
			<figcaption class="mt-2 text-right text-sm text-gray-600 dark:text-gray-400">
				— Marcus Aurelius, Meditations 8.22
			</figcaption>
		</figure>
		<p class="mt-4 mb-4">
			I resonate with the spirit of
			<a class="cactus-link" href="http://slow-science.org/" target="_blank" rel="noopener"
				>Slow Science</a
			>. 
		</p>
		<p class="mt-4 mb-4">
			We live in an age tyrannized by efficiency, outcomes, and speed, to the point that nothing
			lasts and nothing leaves a deep impression. In the midst of noisy bubbles and short-lived
			hype, I hope to take time to think carefully, to doubt, to refine, and to do research that
			is genuinely meaningful and worth remembering.
		</p>
	</section>

	{/* Image modal (reused) */}
	<div
		id="img-modal"
		class="z-70 fixed inset-0 flex hidden items-center justify-center bg-black/40 p-4 backdrop-blur-sm"
	>
		<div class="relative flex max-h-[90vh] max-w-[90vw] items-center justify-center">
			<button
				id="img-modal-close"
				class="absolute right-3 top-3 z-20 inline-flex h-9 w-9 items-center justify-center rounded-full bg-white/90 text-accent shadow-md hover:bg-white"
				aria-label="Close image modal"
			>
				<svg
					xmlns="http://www.w3.org/2000/svg"
					viewBox="0 0 24 24"
					width="16"
					height="16"
					fill="none"
					stroke="currentColor"
					stroke-width="2"
					stroke-linecap="round"
					stroke-linejoin="round"
					aria-hidden="true"
				>
					<line x1="18" y1="6" x2="6" y2="18"></line>
					<line x1="6" y1="6" x2="18" y2="18"></line>
				</svg>
			</button>
			<img
				id="img-modal-img"
				src=""
				alt="Large image"
				class="max-h-[90vh] max-w-full rounded-lg object-contain shadow-xl"
			/>
		</div>
	</div>

	<script>
		document.addEventListener("DOMContentLoaded", () => {
			// toggle abstracts: smooth max-height + opacity, no image panel logic
			document.querySelectorAll(".abstract-btn").forEach((btn) => {
				const id = btn.getAttribute("data-abstract-id");
				const slug = id ? id.replace("abstract-", "") : null;
				const content = slug ? document.getElementById(`abstract-content-${slug}`) : null;
				const chev = btn.querySelector(".chev");
				if (chev && content) {
					const collapsed = content.getAttribute("data-collapsed");
					if (collapsed === "false") chev.classList.add("rotate-180");
					else chev.classList.remove("rotate-180");
				}
				btn.addEventListener("click", () => {
					if (!content) return;
					const collapsed = content.getAttribute("data-collapsed") !== "false";
					if (collapsed) {
						content.style.maxHeight = content.scrollHeight + "px";
						content.style.opacity = "1";
						content.setAttribute("data-collapsed", "false");
						btn.setAttribute("aria-expanded", "true");
						if (chev) chev.classList.add("rotate-180");
						setTimeout(() => {
							if (content.getAttribute("data-collapsed") === "false")
								content.style.maxHeight = "none";
						}, 310);
					} else {
						content.style.maxHeight = content.scrollHeight + "px";
						content.getBoundingClientRect();
						content.style.maxHeight = "0";
						content.style.opacity = "0";
						content.setAttribute("data-collapsed", "true");
						btn.setAttribute("aria-expanded", "false");
						if (chev) chev.classList.remove("rotate-180");
					}
				});
			});

			// Image modal handlers: thumbnails open modal
			const imgModal = document.getElementById("img-modal") as HTMLDivElement | null;
			const imgModalImg = document.getElementById("img-modal-img") as HTMLImageElement | null;
			const imgModalClose = document.getElementById("img-modal-close") as HTMLButtonElement | null;
			function openImgModal(src: string): void {
				if (!imgModal || !imgModalImg) return;
				imgModalImg.src = src;
				imgModal.classList.remove("hidden");
				document.body.style.overflow = "hidden";
				imgModalClose?.focus();
			}
			function closeImgModal(): void {
				if (!imgModal) return;
				imgModal.classList.add("hidden");
				if (imgModalImg) imgModalImg.src = "";
				document.body.style.overflow = "";
			}
			imgModalClose?.addEventListener("click", closeImgModal);
			imgModal?.addEventListener("click", (e) => {
				if (e.target === imgModal) closeImgModal();
			});
			document.addEventListener("keydown", (e) => {
				if (e.key === "Escape") closeImgModal();
			});

			// thumbnail click -> open modal
			document.addEventListener("click", (e) => {
				const target = e.target as EventTarget | null;
				if (!target) return;
				if (target instanceof HTMLImageElement) {
					// thumbnail image
					if (target.classList.contains("pub-thumb-img")) {
						openImgModal(target.src);
						return;
					}
					// inline images (if any) still open modal
					if (target.dataset && target.dataset.inline === "true") {
						openImgModal(target.src);
					}
				}
			});
		});
	</script>
</PageLayout>
